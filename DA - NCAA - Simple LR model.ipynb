{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, model_selection, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>WL</th>\n",
       "      <th>T1_AdjEM</th>\n",
       "      <th>T1_AdjO</th>\n",
       "      <th>T1_AdjD</th>\n",
       "      <th>T1_AdjT</th>\n",
       "      <th>T1_Luck</th>\n",
       "      <th>T1_SOSADjEM</th>\n",
       "      <th>T1_SOSOppO</th>\n",
       "      <th>T1_SOSOppD</th>\n",
       "      <th>T1_NCSOSAdjEM</th>\n",
       "      <th>T2_AdjEM</th>\n",
       "      <th>T2_AdjO</th>\n",
       "      <th>T2_AdjD</th>\n",
       "      <th>T2_AdjT</th>\n",
       "      <th>T2_Luck</th>\n",
       "      <th>T2_SOSADjEM</th>\n",
       "      <th>T2_SOSOppO</th>\n",
       "      <th>T2_SOSOppD</th>\n",
       "      <th>T2_NCSOSAdjEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>98.1</td>\n",
       "      <td>106.8</td>\n",
       "      <td>74.2</td>\n",
       "      <td>0.097</td>\n",
       "      <td>-15.24</td>\n",
       "      <td>93.2</td>\n",
       "      <td>108.4</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>-1.20</td>\n",
       "      <td>98.6</td>\n",
       "      <td>99.8</td>\n",
       "      <td>68.9</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>98.7</td>\n",
       "      <td>102.6</td>\n",
       "      <td>-1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>18.73</td>\n",
       "      <td>111.9</td>\n",
       "      <td>93.2</td>\n",
       "      <td>69.1</td>\n",
       "      <td>0.076</td>\n",
       "      <td>9.90</td>\n",
       "      <td>106.3</td>\n",
       "      <td>96.4</td>\n",
       "      <td>2.49</td>\n",
       "      <td>-2.48</td>\n",
       "      <td>97.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>71.6</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-5.12</td>\n",
       "      <td>100.3</td>\n",
       "      <td>105.4</td>\n",
       "      <td>-5.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>20.52</td>\n",
       "      <td>117.2</td>\n",
       "      <td>96.7</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.080</td>\n",
       "      <td>14.48</td>\n",
       "      <td>109.9</td>\n",
       "      <td>95.4</td>\n",
       "      <td>18.68</td>\n",
       "      <td>8.90</td>\n",
       "      <td>106.1</td>\n",
       "      <td>97.2</td>\n",
       "      <td>64.8</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.23</td>\n",
       "      <td>101.4</td>\n",
       "      <td>101.2</td>\n",
       "      <td>2.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>34.14</td>\n",
       "      <td>118.9</td>\n",
       "      <td>84.7</td>\n",
       "      <td>75.9</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>10.09</td>\n",
       "      <td>107.8</td>\n",
       "      <td>97.7</td>\n",
       "      <td>7.52</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>95.2</td>\n",
       "      <td>100.5</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>95.4</td>\n",
       "      <td>106.2</td>\n",
       "      <td>-1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>24.80</td>\n",
       "      <td>112.0</td>\n",
       "      <td>87.2</td>\n",
       "      <td>66.8</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>13.66</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.4</td>\n",
       "      <td>13.51</td>\n",
       "      <td>14.11</td>\n",
       "      <td>109.4</td>\n",
       "      <td>95.2</td>\n",
       "      <td>63.6</td>\n",
       "      <td>0.011</td>\n",
       "      <td>5.87</td>\n",
       "      <td>104.9</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  WL  T1_AdjEM  T1_AdjO  T1_AdjD  T1_AdjT  T1_Luck  T1_SOSADjEM  \\\n",
       "0    2002   0     -8.69     98.1    106.8     74.2    0.097       -15.24   \n",
       "1    2002   1     18.73    111.9     93.2     69.1    0.076         9.90   \n",
       "2    2002   1     20.52    117.2     96.7     73.0    0.080        14.48   \n",
       "3    2002   1     34.14    118.9     84.7     75.9   -0.027        10.09   \n",
       "4    2002   1     24.80    112.0     87.2     66.8   -0.049        13.66   \n",
       "\n",
       "   T1_SOSOppO  T1_SOSOppD  T1_NCSOSAdjEM  T2_AdjEM  T2_AdjO  T2_AdjD  T2_AdjT  \\\n",
       "0        93.2       108.4          -2.29     -1.20     98.6     99.8     68.9   \n",
       "1       106.3        96.4           2.49     -2.48     97.5    100.0     71.6   \n",
       "2       109.9        95.4          18.68      8.90    106.1     97.2     64.8   \n",
       "3       107.8        97.7           7.52     -5.33     95.2    100.5     68.0   \n",
       "4       109.0        95.4          13.51     14.11    109.4     95.2     63.6   \n",
       "\n",
       "   T2_Luck  T2_SOSADjEM  T2_SOSOppO  T2_SOSOppD  T2_NCSOSAdjEM  \n",
       "0   -0.070        -3.90        98.7       102.6          -1.15  \n",
       "1    0.061        -5.12       100.3       105.4          -5.36  \n",
       "2   -0.058         0.23       101.4       101.2           2.86  \n",
       "3    0.039       -10.81        95.4       106.2          -1.40  \n",
       "4    0.011         5.87       104.9        99.0           3.57  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/All_PCBR_For_2002_2016_Matchup.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yTrain = data.WL.ravel()\n",
    "xTrain = data.drop(['WL','Season'], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Cross validation\n",
    "I used logistic regression to build up my model and use log loss to evaluate the result. And I also show the score if all probabilities of each matchup is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0.479049868237\n",
      "0.5: 0.69314718056\n",
      "pred: 0.474212795495\n",
      "0.5: 0.69314718056\n",
      "pred: 0.480857427448\n",
      "0.5: 0.69314718056\n",
      "pred: 0.491307654836\n",
      "0.5: 0.69314718056\n",
      "pred: 0.475196629867\n",
      "0.5: 0.69314718056\n",
      "pred: 0.483288241771\n",
      "0.5: 0.69314718056\n",
      "pred: 0.480505346175\n",
      "0.5: 0.69314718056\n",
      "pred: 0.480052517123\n",
      "0.5: 0.69314718056\n",
      "pred: 0.472774758669\n",
      "0.5: 0.69314718056\n",
      "pred: 0.474231492827\n",
      "0.5: 0.69314718056\n"
     ]
    }
   ],
   "source": [
    "kf = model_selection.KFold(n_splits=10, shuffle=True)\n",
    "for train, test in kf.split(xTrain):\n",
    "    #print(\"%s %s\" % (train, test))\n",
    "    #print(xTrain[train])\n",
    "    logi = linear_model.LogisticRegression()\n",
    "    logi.fit(xTrain[train],yTrain[train])\n",
    "    pred = logi.predict_proba(xTrain[test])\n",
    "    print(\"pred: \" + str(metrics.log_loss(yTrain[test],pred[:,1])))\n",
    "    print(\"0.5: \" + str(metrics.log_loss(yTrain[test],np.ones(test.shape[0]) * 0.5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
